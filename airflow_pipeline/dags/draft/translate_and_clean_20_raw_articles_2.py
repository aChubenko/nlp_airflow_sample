from airflow.decorators import dag, task
from datetime import datetime, timedelta
import psycopg2
import boto3
import os
import logging
import time
from io import BytesIO
from PyPDF2 import PdfReader
from openai import OpenAI

logger = logging.getLogger(__name__)

PG_CONN = {
    "dbname": "arxiv",
    "user": "airflow",
    "password": "airflow",
    "host": "postgres",
    "port": 5432,
}

MINIO_ENDPOINT = "http://minio:9000"
MINIO_INPUT_BUCKET = "raw-articles"
MINIO_OUTPUT_BUCKET = "cleaned-articles"
MINIO_ACCESS_KEY = os.getenv("MINIO_ROOT_USER", "minioadmin")
MINIO_SECRET_KEY = os.getenv("MINIO_ROOT_PASSWORD", "minioadmin")

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

@dag(
    dag_id="clean_success_articles_to_s3_2",
    schedule_interval=None,
    start_date=datetime(2025, 1, 1),
    catchup=False,
    max_active_runs=1,
    default_args={"retries": 1, "retry_delay": timedelta(minutes=5)},
    is_paused_upon_creation=True,
    tags=["draft", "experimental"]
)
def clean_success_articles_to_s3_2():

    @task()
    def get_successful_articles(**context):
        self = context['ti']
        self.log.info("üì• Fetching up to 20 successful articles from PostgreSQL...")
        with psycopg2.connect(**PG_CONN) as conn:
            with conn.cursor() as cur:
                cur.execute("""
                    SELECT identifier FROM arxiv_articles
                    WHERE status = 'success'
                    LIMIT 20;
                """)
                rows = cur.fetchall()
                self.log.info(f"üîé Retrieved {len(rows)} articles to clean.")
                return [r[0] for r in rows]

    @task()
    def clean_and_upload_sequential(identifiers: list[str], **context):
        self = context['ti']
        s3 = boto3.client(
            "s3",
            endpoint_url=MINIO_ENDPOINT,
            aws_access_key_id=MINIO_ACCESS_KEY,
            aws_secret_access_key=MINIO_SECRET_KEY
        )

        for identifier in identifiers:
            self.log.info(f"‚¨áÔ∏è Downloading PDF for {identifier}")
            try:
                response = s3.get_object(Bucket=MINIO_INPUT_BUCKET, Key=f"{identifier}.pdf")
                pdf_bytes = response['Body'].read()

                reader = PdfReader(BytesIO(pdf_bytes))
                full_text = "\n".join([page.extract_text() or "" for page in reader.pages]).strip()

                if not full_text:
                    raise ValueError("PDF contains no extractable text")

                self.log.info(f"üåê Translating {identifier} via OpenAI")
                response = client.chat.completions.create(
                    model="gpt-4-turbo",
                    messages=[
                        {"role": "system", "content": "–ü–µ—Ä–µ–∫–ª–∞–¥–∏ –Ω–∞ —É–∫—Ä–∞—ó–Ω—Å—å–∫—É –º–æ–≤—É –Ω–∞—É–∫–æ–≤–∏–π —Ç–µ–∫—Å—Ç –Ω–∏–∂—á–µ. –ó–±–µ—Ä—ñ–≥–∞–π —Å—Ç–∏–ª—å —Ç–∞ —Ç–µ—Ä–º—ñ–Ω–∏."},
                        {"role": "user", "content": full_text[:8000]}
                    ],
                    temperature=0.3
                )
                translated_text = response.choices[0].message.content.strip()

                s3.put_object(Bucket=MINIO_OUTPUT_BUCKET, Key=f"{identifier}.txt", Body=translated_text.encode("utf-8"))
                self.log.info(f"‚úÖ Uploaded {identifier} to cleaned bucket")

            except Exception as e:
                self.log.error(f"‚ùå Failed to process {identifier}: {e}")

            self.log.info("‚è≥ Waiting 60 seconds before next...")
            time.sleep(60)

    identifiers = get_successful_articles()
    clean_and_upload_sequential(identifiers)

dag = clean_success_articles_to_s3_2()
